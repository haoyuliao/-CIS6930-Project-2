{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "talented-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n",
    "import torch, torchvision, os, cv2\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from torch.nn import Tanh, Linear, ReLU, Sequential, Conv2d, MaxPool2d, Sigmoid, BatchNorm2d, Flatten, ConvTranspose2d\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class augementData(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        item = self.transform(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "round-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./face_images/\"\n",
    "_, _, files = next(os.walk(img_dir))\n",
    "dataBGR = []\n",
    "dataLAB = []\n",
    "for f1 in files:\n",
    "    img = cv2.imread(img_dir+f1)\n",
    "    dataBGR.append(img)\n",
    "    imageLAB = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    dataLAB.append(imageLAB)\n",
    "    \n",
    "dataBGR = np.array(dataBGR, dtype = np.float32) #Change data type into float 32.\n",
    "dataLAB = np.array(dataLAB, dtype = np.float32) #Change data type into float 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stunning-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data types and what they mean!!!\n",
    "# https://scikit-image.org/docs/dev/user_guide/data_types.html\n",
    "# Data type\n",
    "# Range\n",
    "# uint8\n",
    "# 0 to 255\n",
    "# uint16\n",
    "# 0 to 65535\n",
    "# uint32\n",
    "# 0 to 232 - 1\n",
    "# float\n",
    "# -1 to 1 or 0 to 1\n",
    "# int8\n",
    "# -128 to 127\n",
    "# int16\n",
    "# -32768 to 32767\n",
    "# int32\n",
    "# -231 to 231 - 1\n",
    "\n",
    "#According to openCV documents, the range of L*, B*, C* in openCV are expressed as:\n",
    "#Articel ource https://rodrigoberriel.com/2014/11/opencv-color-spaces-splitting-channels/#:~:text=The%20Lab%20ranges%20are%3A,(1%20%3E%20L%20%3E%20255)\n",
    "#0 > L > 100 ⇒ OpenCV range = L*255/100 (1 > L > 255)\n",
    "#-127 > a > 127 ⇒ OpenCV range = a + 128 (1 > a > 255)\n",
    "#-127 > b > 127 ⇒ OpenCV range = b + 128 (1 > b > 255)\n",
    "\n",
    "dataLAB = dataLAB / 255 #Change reange into [0, 1]\n",
    "dataLAB = np.moveaxis(dataLAB, -1, 1) #Reshape channeL from [B, H, W, C] to [B, C, H, W]\n",
    "dataLAB = dataLAB[torch.randperm(dataLAB.shape[0],generator=torch.random.manual_seed(42))] #shuffle with random seed 42 to make sure each round with same samples pool.\n",
    "lengths = [int(len(dataLAB)*0.9), int(len(dataLAB)*0.1)]\n",
    "trainLAB, testLAB = random_split(dataLAB, lengths ,generator=torch.random.manual_seed(42)) #Shuffle data with random seed 42 before split train and test\n",
    "trainLAB = np.array(trainLAB)\n",
    "testLAB = np.array(testLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "trainLABTensor = torch.tensor(trainLAB)\n",
    "trainLABTensorx10 = torch.clone(trainLABTensor)\n",
    "for i in range(9):\n",
    "    trainLABTensorx10 = torch.cat((trainLABTensorx10, torch.clone(trainLABTensor)), 0)\n",
    "    \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomResizedCrop((128,128),scale=(0.6, 1.0)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "trainLABTensorx10Aug = augementData(trainLABTensorx10,transform) #Augement train data.\n",
    "\n",
    "trainData = [] \n",
    "trainLx10 = [] #Input L*\n",
    "trainAx10 = [] #Matrix a* #Label\n",
    "trainBx10 = [] #Matrix b* #Label\n",
    "trainAvg_ax10 = [] #scalar mean a* #Label\n",
    "trainAvg_bx10 = [] #scalar mean b* #Label\n",
    "i = 0\n",
    "for t in trainLABTensorx10Aug:\n",
    "    meanA = torch.mean(t[1])\n",
    "    meanB = torch.mean(t[2]) #trainLABTensorx10Aug[i,0,:,:]\n",
    "    #print(np.array(trainLABTensorx10Aug[i,0,:,:]).shape)\n",
    "    wh = len(t[0])\n",
    "    trainData.append([torch.reshape(t[0], (1, wh, wh)), torch.tensor([meanA, meanB]), \n",
    "                      torch.tensor([np.array(t[1]), np.array(t[2])])])\n",
    "    trainLx10.append(t[0])\n",
    "    trainAx10.append(t[1])\n",
    "    trainBx10.append(t[2])\n",
    "    trainAvg_ax10.append(meanA)\n",
    "    trainAvg_bx10.append(meanB)\n",
    "    i+=1\n",
    "trainAvg_ax10, trainAvg_bx10 = np.array(trainAvg_ax10), np.array(trainAvg_bx10)     \n",
    "trainlabelAvgABx10 = np.stack((trainAvg_ax10, trainAvg_bx10),axis=1)\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, shuffle=True, batch_size=100)\n",
    "\n",
    "testDataL = testLAB[:,0,:,:] #Input L\n",
    "testDataL = testDataL.reshape((testDataL.shape[0],1,testDataL.shape[1],testDataL.shape[2]))\n",
    "testDataA = testLAB[:,1,:,:] #a* matrix\n",
    "testDataA = testDataA.reshape((testDataA.shape[0],1,testDataA.shape[1],testDataA.shape[2]))\n",
    "testDataB = testLAB[:,2,:,:] #b* matrix\n",
    "testDataB = testDataB.reshape((testDataB.shape[0],1,testDataB.shape[1],testDataB.shape[2]))\n",
    "testAvg_a = testLAB.mean(axis=(2,3))[:,1] #Get label mean of each a* \n",
    "testAvg_b = testLAB.mean(axis=(2,3))[:,2] #Get label mean of each b*\n",
    "testDataL = torch.tensor(testDataL) #Test input\n",
    "testlabelAvgAB = np.stack((testAvg_a, testAvg_b),axis=1)\n",
    "testlabelAvgAB = torch.tensor(testlabelAvgAB) #Test output label\n",
    "\n",
    "testData = []\n",
    "for i in range(len(testDataL)):\n",
    "    testData.append([testDataL[i], testlabelAvgAB[i], torch.tensor([testDataA[i][0], testDataB[i][0]])])\n",
    "testLoader = torch.utils.data.DataLoader(testData, shuffle=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complicated-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential(\n",
    "            # Defining 1st 2D convolution layer\n",
    "            Conv2d(1, 3, kernel_size=3, stride=1, padding=1), #128@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 2nd 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #64@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 3rd 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #32@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 4th 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #16@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 5th 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #8@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 6th 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #4@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining 7th 2D convolution layer\n",
    "            Conv2d(3, 3, kernel_size=3, stride=1, padding=1), #2@3\n",
    "            BatchNorm2d(3),\n",
    "            ReLU(inplace=True),\n",
    "            #MaxPool2d(kernel_size=2, stride=2),\n",
    "            Flatten(),\n",
    "            Linear(3 * 2 * 2, 2)\n",
    "        )\n",
    "net = net.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clean-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1, MSE_Loss: 0.0163966381\n",
      "Train epoch: 2, MSE_Loss: 0.0002780030\n",
      "Train epoch: 3, MSE_Loss: 0.0002476326\n",
      "Train epoch: 4, MSE_Loss: 0.0002321774\n",
      "Train epoch: 5, MSE_Loss: 0.0002237800\n",
      "Train epoch: 6, MSE_Loss: 0.0002201383\n",
      "Train epoch: 7, MSE_Loss: 0.0002148526\n",
      "Train epoch: 8, MSE_Loss: 0.0002089003\n",
      "Train epoch: 9, MSE_Loss: 0.0002060139\n",
      "Train epoch: 10, MSE_Loss: 0.0002041849\n",
      "Train epoch: 11, MSE_Loss: 0.0002015594\n",
      "Train epoch: 12, MSE_Loss: 0.0002008114\n",
      "Train epoch: 13, MSE_Loss: 0.0001989846\n",
      "Train epoch: 14, MSE_Loss: 0.0001992389\n",
      "Train epoch: 15, MSE_Loss: 0.0001983321\n",
      "Train epoch: 16, MSE_Loss: 0.0001985289\n",
      "Train epoch: 17, MSE_Loss: 0.0001937668\n",
      "Train epoch: 18, MSE_Loss: 0.0001933088\n",
      "Train epoch: 19, MSE_Loss: 0.0001923591\n",
      "Train epoch: 20, MSE_Loss: 0.0001918973\n",
      "Train epoch: 21, MSE_Loss: 0.0001912232\n",
      "Train epoch: 22, MSE_Loss: 0.0001883845\n",
      "Train epoch: 23, MSE_Loss: 0.0001885180\n",
      "Train epoch: 24, MSE_Loss: 0.0001878422\n",
      "Train epoch: 25, MSE_Loss: 0.0001876751\n",
      "Train epoch: 26, MSE_Loss: 0.0001853403\n",
      "Train epoch: 27, MSE_Loss: 0.0001851565\n",
      "Train epoch: 28, MSE_Loss: 0.0001837510\n",
      "Train epoch: 29, MSE_Loss: 0.0001832321\n",
      "Train epoch: 30, MSE_Loss: 0.0001821900\n",
      "Train epoch: 31, MSE_Loss: 0.0001801281\n",
      "Train epoch: 32, MSE_Loss: 0.0001791096\n",
      "Train epoch: 33, MSE_Loss: 0.0001795170\n",
      "Train epoch: 34, MSE_Loss: 0.0001780917\n",
      "Train epoch: 35, MSE_Loss: 0.0001753852\n",
      "Train epoch: 36, MSE_Loss: 0.0001761870\n",
      "Train epoch: 37, MSE_Loss: 0.0001753237\n",
      "Train epoch: 38, MSE_Loss: 0.0001731432\n",
      "Train epoch: 39, MSE_Loss: 0.0001704121\n",
      "Train epoch: 40, MSE_Loss: 0.0001693895\n",
      "Train epoch: 41, MSE_Loss: 0.0001743763\n",
      "Train epoch: 42, MSE_Loss: 0.0001681101\n",
      "Train epoch: 43, MSE_Loss: 0.0001672457\n",
      "Train epoch: 44, MSE_Loss: 0.0001656955\n",
      "Train epoch: 45, MSE_Loss: 0.0001671927\n",
      "Train epoch: 46, MSE_Loss: 0.0001645955\n",
      "Train epoch: 47, MSE_Loss: 0.0001629893\n",
      "Train epoch: 48, MSE_Loss: 0.0001630705\n",
      "Train epoch: 49, MSE_Loss: 0.0001629464\n",
      "Train epoch: 50, MSE_Loss: 0.0001635731\n",
      "Train epoch: 51, MSE_Loss: 0.0001649145\n",
      "Train epoch: 52, MSE_Loss: 0.0001609487\n",
      "Train epoch: 53, MSE_Loss: 0.0001605132\n",
      "Train epoch: 54, MSE_Loss: 0.0001641062\n",
      "Train epoch: 55, MSE_Loss: 0.0001600068\n",
      "Train epoch: 56, MSE_Loss: 0.0001588271\n",
      "Train epoch: 57, MSE_Loss: 0.0001620921\n",
      "Train epoch: 58, MSE_Loss: 0.0001584031\n",
      "Train epoch: 59, MSE_Loss: 0.0001577230\n",
      "Train epoch: 60, MSE_Loss: 0.0001598362\n",
      "Train epoch: 61, MSE_Loss: 0.0001595731\n",
      "Train epoch: 62, MSE_Loss: 0.0001619281\n",
      "Train epoch: 63, MSE_Loss: 0.0001580586\n",
      "Train epoch: 64, MSE_Loss: 0.0001579924\n",
      "Train epoch: 65, MSE_Loss: 0.0001558016\n",
      "Train epoch: 66, MSE_Loss: 0.0001557640\n",
      "Train epoch: 67, MSE_Loss: 0.0001555799\n",
      "Train epoch: 68, MSE_Loss: 0.0001569218\n",
      "Train epoch: 69, MSE_Loss: 0.0001570147\n",
      "Train epoch: 70, MSE_Loss: 0.0001558678\n",
      "Train epoch: 71, MSE_Loss: 0.0001551502\n",
      "Train epoch: 72, MSE_Loss: 0.0001553943\n",
      "Train epoch: 73, MSE_Loss: 0.0001554961\n",
      "Train epoch: 74, MSE_Loss: 0.0001541706\n",
      "Train epoch: 75, MSE_Loss: 0.0001547660\n",
      "Train epoch: 76, MSE_Loss: 0.0001551246\n",
      "Train epoch: 77, MSE_Loss: 0.0001537306\n",
      "Train epoch: 78, MSE_Loss: 0.0001532974\n",
      "Train epoch: 79, MSE_Loss: 0.0001546181\n",
      "Train epoch: 80, MSE_Loss: 0.0001547978\n",
      "Train epoch: 81, MSE_Loss: 0.0001546151\n",
      "Train epoch: 82, MSE_Loss: 0.0001526694\n",
      "Train epoch: 83, MSE_Loss: 0.0001527176\n",
      "Train epoch: 84, MSE_Loss: 0.0001533593\n",
      "Train epoch: 85, MSE_Loss: 0.0001530322\n",
      "Train epoch: 86, MSE_Loss: 0.0001537085\n",
      "Train epoch: 87, MSE_Loss: 0.0001521386\n",
      "Train epoch: 88, MSE_Loss: 0.0001533251\n",
      "Train epoch: 89, MSE_Loss: 0.0001536326\n",
      "Train epoch: 90, MSE_Loss: 0.0001520061\n",
      "Train epoch: 91, MSE_Loss: 0.0001539859\n",
      "Train epoch: 92, MSE_Loss: 0.0001528077\n",
      "Train epoch: 93, MSE_Loss: 0.0001537130\n",
      "Train epoch: 94, MSE_Loss: 0.0001524648\n",
      "Train epoch: 95, MSE_Loss: 0.0001513698\n",
      "Train epoch: 96, MSE_Loss: 0.0001522353\n",
      "Train epoch: 97, MSE_Loss: 0.0001516256\n",
      "Train epoch: 98, MSE_Loss: 0.0001514987\n",
      "Train epoch: 99, MSE_Loss: 0.0001526387\n",
      "Train epoch: 100, MSE_Loss: 0.0001532666\n",
      "Train epoch: 101, MSE_Loss: 0.0001515133\n",
      "Train epoch: 102, MSE_Loss: 0.0001514757\n",
      "Train epoch: 103, MSE_Loss: 0.0001507283\n",
      "Train epoch: 104, MSE_Loss: 0.0001518646\n",
      "Train epoch: 105, MSE_Loss: 0.0001517293\n",
      "Train epoch: 106, MSE_Loss: 0.0001527261\n",
      "Train epoch: 107, MSE_Loss: 0.0001501150\n",
      "Train epoch: 108, MSE_Loss: 0.0001507008\n",
      "Train epoch: 109, MSE_Loss: 0.0001508276\n",
      "Train epoch: 110, MSE_Loss: 0.0001524972\n",
      "Train epoch: 111, MSE_Loss: 0.0001514783\n",
      "Train epoch: 112, MSE_Loss: 0.0001510219\n",
      "Train epoch: 113, MSE_Loss: 0.0001504027\n",
      "Train epoch: 114, MSE_Loss: 0.0001500869\n",
      "Train epoch: 115, MSE_Loss: 0.0001501320\n",
      "Train epoch: 116, MSE_Loss: 0.0001520970\n",
      "Train epoch: 117, MSE_Loss: 0.0001502107\n",
      "Train epoch: 118, MSE_Loss: 0.0001492500\n",
      "Train epoch: 119, MSE_Loss: 0.0001496331\n",
      "Train epoch: 120, MSE_Loss: 0.0001498357\n",
      "Train epoch: 121, MSE_Loss: 0.0001516238\n",
      "Train epoch: 122, MSE_Loss: 0.0001500644\n",
      "Train epoch: 123, MSE_Loss: 0.0001502877\n",
      "Train epoch: 124, MSE_Loss: 0.0001503477\n",
      "Train epoch: 125, MSE_Loss: 0.0001484639\n",
      "Train epoch: 126, MSE_Loss: 0.0001496671\n",
      "Train epoch: 127, MSE_Loss: 0.0001487849\n",
      "Train epoch: 128, MSE_Loss: 0.0001490646\n",
      "Train epoch: 129, MSE_Loss: 0.0001506667\n",
      "Train epoch: 130, MSE_Loss: 0.0001494681\n",
      "Train epoch: 131, MSE_Loss: 0.0001475319\n",
      "Train epoch: 132, MSE_Loss: 0.0001478462\n",
      "Train epoch: 133, MSE_Loss: 0.0001495218\n",
      "Train epoch: 134, MSE_Loss: 0.0001484614\n",
      "Train epoch: 135, MSE_Loss: 0.0001479897\n",
      "Train epoch: 136, MSE_Loss: 0.0001482013\n",
      "Train epoch: 137, MSE_Loss: 0.0001479402\n",
      "Train epoch: 138, MSE_Loss: 0.0001510386\n",
      "Train epoch: 139, MSE_Loss: 0.0001481759\n",
      "Train epoch: 140, MSE_Loss: 0.0001475951\n",
      "Train epoch: 141, MSE_Loss: 0.0001473522\n",
      "Train epoch: 142, MSE_Loss: 0.0001471915\n",
      "Train epoch: 143, MSE_Loss: 0.0001481872\n",
      "Train epoch: 144, MSE_Loss: 0.0001480084\n",
      "Train epoch: 145, MSE_Loss: 0.0001484913\n",
      "Train epoch: 146, MSE_Loss: 0.0001487972\n",
      "Train epoch: 147, MSE_Loss: 0.0001477038\n",
      "Train epoch: 148, MSE_Loss: 0.0001483140\n",
      "Train epoch: 149, MSE_Loss: 0.0001475412\n",
      "Train epoch: 150, MSE_Loss: 0.0001479658\n",
      "Train epoch: 151, MSE_Loss: 0.0001494714\n",
      "Train epoch: 152, MSE_Loss: 0.0001470916\n",
      "Train epoch: 153, MSE_Loss: 0.0001468450\n",
      "Train epoch: 154, MSE_Loss: 0.0001475903\n",
      "Train epoch: 155, MSE_Loss: 0.0001476239\n",
      "Train epoch: 156, MSE_Loss: 0.0001503157\n",
      "Train epoch: 157, MSE_Loss: 0.0001496443\n",
      "Train epoch: 158, MSE_Loss: 0.0001474684\n",
      "Train epoch: 159, MSE_Loss: 0.0001494173\n",
      "Train epoch: 160, MSE_Loss: 0.0001475738\n",
      "Train epoch: 161, MSE_Loss: 0.0001461871\n",
      "Train epoch: 162, MSE_Loss: 0.0001468583\n",
      "Train epoch: 163, MSE_Loss: 0.0001471110\n",
      "Train epoch: 164, MSE_Loss: 0.0001468746\n",
      "Train epoch: 165, MSE_Loss: 0.0001514102\n",
      "Train epoch: 166, MSE_Loss: 0.0001505515\n",
      "Train epoch: 167, MSE_Loss: 0.0001487849\n",
      "Train epoch: 168, MSE_Loss: 0.0001469002\n",
      "Train epoch: 169, MSE_Loss: 0.0001465726\n",
      "Train epoch: 170, MSE_Loss: 0.0001465253\n",
      "Train epoch: 171, MSE_Loss: 0.0001458801\n",
      "Train epoch: 172, MSE_Loss: 0.0001459497\n",
      "Train epoch: 173, MSE_Loss: 0.0001462813\n",
      "Train epoch: 174, MSE_Loss: 0.0001459199\n",
      "Train epoch: 175, MSE_Loss: 0.0001457667\n",
      "Train epoch: 176, MSE_Loss: 0.0001459424\n",
      "Train epoch: 177, MSE_Loss: 0.0001466827\n",
      "Train epoch: 178, MSE_Loss: 0.0001457254\n",
      "Train epoch: 179, MSE_Loss: 0.0001459903\n",
      "Train epoch: 180, MSE_Loss: 0.0001460304\n",
      "Train epoch: 181, MSE_Loss: 0.0001481866\n",
      "Train epoch: 182, MSE_Loss: 0.0001470085\n",
      "Train epoch: 183, MSE_Loss: 0.0001456861\n",
      "Train epoch: 184, MSE_Loss: 0.0001453779\n",
      "Train epoch: 185, MSE_Loss: 0.0001452497\n",
      "Train epoch: 186, MSE_Loss: 0.0001466759\n",
      "Train epoch: 187, MSE_Loss: 0.0001452235\n",
      "Train epoch: 188, MSE_Loss: 0.0001466995\n",
      "Train epoch: 189, MSE_Loss: 0.0001450851\n",
      "Train epoch: 190, MSE_Loss: 0.0001449671\n",
      "Train epoch: 191, MSE_Loss: 0.0001457409\n",
      "Train epoch: 192, MSE_Loss: 0.0001457184\n",
      "Train epoch: 193, MSE_Loss: 0.0001479407\n",
      "Train epoch: 194, MSE_Loss: 0.0001464443\n",
      "Train epoch: 195, MSE_Loss: 0.0001481455\n",
      "Train epoch: 196, MSE_Loss: 0.0001460779\n",
      "Train epoch: 197, MSE_Loss: 0.0001452340\n",
      "Train epoch: 198, MSE_Loss: 0.0001456047\n",
      "Train epoch: 199, MSE_Loss: 0.0001471670\n",
      "Train epoch: 200, MSE_Loss: 0.0001475587\n",
      "Train epoch: 201, MSE_Loss: 0.0001481389\n",
      "Train epoch: 202, MSE_Loss: 0.0001454577\n",
      "Train epoch: 203, MSE_Loss: 0.0001443524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 204, MSE_Loss: 0.0001459961\n",
      "Train epoch: 205, MSE_Loss: 0.0001446468\n",
      "Train epoch: 206, MSE_Loss: 0.0001472298\n",
      "Train epoch: 207, MSE_Loss: 0.0001447733\n",
      "Train epoch: 208, MSE_Loss: 0.0001434078\n",
      "Train epoch: 209, MSE_Loss: 0.0001449136\n",
      "Train epoch: 210, MSE_Loss: 0.0001436171\n",
      "Train epoch: 211, MSE_Loss: 0.0001447326\n",
      "Train epoch: 212, MSE_Loss: 0.0001466344\n",
      "Train epoch: 213, MSE_Loss: 0.0001464179\n",
      "Train epoch: 214, MSE_Loss: 0.0001432697\n",
      "Train epoch: 215, MSE_Loss: 0.0001439054\n",
      "Train epoch: 216, MSE_Loss: 0.0001449756\n",
      "Train epoch: 217, MSE_Loss: 0.0001436612\n",
      "Train epoch: 218, MSE_Loss: 0.0001456536\n",
      "Train epoch: 219, MSE_Loss: 0.0001473161\n",
      "Train epoch: 220, MSE_Loss: 0.0001448784\n",
      "Train epoch: 221, MSE_Loss: 0.0001440014\n",
      "Train epoch: 222, MSE_Loss: 0.0001467398\n",
      "Train epoch: 223, MSE_Loss: 0.0001438161\n",
      "Train epoch: 224, MSE_Loss: 0.0001431143\n",
      "Train epoch: 225, MSE_Loss: 0.0001448918\n",
      "Train epoch: 226, MSE_Loss: 0.0001437157\n",
      "Train epoch: 227, MSE_Loss: 0.0001438850\n",
      "Train epoch: 228, MSE_Loss: 0.0001414954\n",
      "Train epoch: 229, MSE_Loss: 0.0001443086\n",
      "Train epoch: 230, MSE_Loss: 0.0001436494\n",
      "Train epoch: 231, MSE_Loss: 0.0001428908\n",
      "Train epoch: 232, MSE_Loss: 0.0001429910\n",
      "Train epoch: 233, MSE_Loss: 0.0001434844\n",
      "Train epoch: 234, MSE_Loss: 0.0001451240\n",
      "Train epoch: 235, MSE_Loss: 0.0001427026\n",
      "Train epoch: 236, MSE_Loss: 0.0001437550\n",
      "Train epoch: 237, MSE_Loss: 0.0001455009\n",
      "Train epoch: 238, MSE_Loss: 0.0001442294\n",
      "Train epoch: 239, MSE_Loss: 0.0001467807\n",
      "Train epoch: 240, MSE_Loss: 0.0001424549\n",
      "Train epoch: 241, MSE_Loss: 0.0001432553\n",
      "Train epoch: 242, MSE_Loss: 0.0001422166\n",
      "Train epoch: 243, MSE_Loss: 0.0001429998\n",
      "Train epoch: 244, MSE_Loss: 0.0001429847\n",
      "Train epoch: 245, MSE_Loss: 0.0001428620\n",
      "Train epoch: 246, MSE_Loss: 0.0001432318\n",
      "Train epoch: 247, MSE_Loss: 0.0001417836\n",
      "Train epoch: 248, MSE_Loss: 0.0001431275\n",
      "Train epoch: 249, MSE_Loss: 0.0001414486\n",
      "Train epoch: 250, MSE_Loss: 0.0001424067\n",
      "Train epoch: 251, MSE_Loss: 0.0001424331\n",
      "Train epoch: 252, MSE_Loss: 0.0001430756\n",
      "Train epoch: 253, MSE_Loss: 0.0001422115\n",
      "Train epoch: 254, MSE_Loss: 0.0001410939\n",
      "Train epoch: 255, MSE_Loss: 0.0001416771\n",
      "Train epoch: 256, MSE_Loss: 0.0001410850\n",
      "Train epoch: 257, MSE_Loss: 0.0001438478\n",
      "Train epoch: 258, MSE_Loss: 0.0001420901\n",
      "Train epoch: 259, MSE_Loss: 0.0001425038\n",
      "Train epoch: 260, MSE_Loss: 0.0001415462\n",
      "Train epoch: 261, MSE_Loss: 0.0001427478\n",
      "Train epoch: 262, MSE_Loss: 0.0001433319\n",
      "Train epoch: 263, MSE_Loss: 0.0001422013\n",
      "Train epoch: 264, MSE_Loss: 0.0001423025\n",
      "Train epoch: 265, MSE_Loss: 0.0001420089\n",
      "Train epoch: 266, MSE_Loss: 0.0001421586\n",
      "Train epoch: 267, MSE_Loss: 0.0001423626\n",
      "Train epoch: 268, MSE_Loss: 0.0001413856\n",
      "Train epoch: 269, MSE_Loss: 0.0001407812\n",
      "Train epoch: 270, MSE_Loss: 0.0001424625\n",
      "Train epoch: 271, MSE_Loss: 0.0001410075\n",
      "Train epoch: 272, MSE_Loss: 0.0001421327\n",
      "Train epoch: 273, MSE_Loss: 0.0001406808\n",
      "Train epoch: 274, MSE_Loss: 0.0001424120\n",
      "Train epoch: 275, MSE_Loss: 0.0001398149\n",
      "Train epoch: 276, MSE_Loss: 0.0001431679\n",
      "Train epoch: 277, MSE_Loss: 0.0001409314\n",
      "Train epoch: 278, MSE_Loss: 0.0001413823\n",
      "Train epoch: 279, MSE_Loss: 0.0001406813\n",
      "Train epoch: 280, MSE_Loss: 0.0001420991\n",
      "Train epoch: 281, MSE_Loss: 0.0001423887\n",
      "Train epoch: 282, MSE_Loss: 0.0001412915\n",
      "Train epoch: 283, MSE_Loss: 0.0001400048\n",
      "Train epoch: 284, MSE_Loss: 0.0001398111\n",
      "Train epoch: 285, MSE_Loss: 0.0001422218\n",
      "Train epoch: 286, MSE_Loss: 0.0001416826\n",
      "Train epoch: 287, MSE_Loss: 0.0001399032\n",
      "Train epoch: 288, MSE_Loss: 0.0001412740\n",
      "Train epoch: 289, MSE_Loss: 0.0001398504\n",
      "Train epoch: 290, MSE_Loss: 0.0001422898\n",
      "Train epoch: 291, MSE_Loss: 0.0001396441\n",
      "Train epoch: 292, MSE_Loss: 0.0001397316\n",
      "Train epoch: 293, MSE_Loss: 0.0001393528\n",
      "Train epoch: 294, MSE_Loss: 0.0001417837\n",
      "Train epoch: 295, MSE_Loss: 0.0001397512\n",
      "Train epoch: 296, MSE_Loss: 0.0001414424\n",
      "Train epoch: 297, MSE_Loss: 0.0001391722\n",
      "Train epoch: 298, MSE_Loss: 0.0001396623\n",
      "Train epoch: 299, MSE_Loss: 0.0001394958\n",
      "Train epoch: 300, MSE_Loss: 0.0001396248\n",
      "Train epoch: 301, MSE_Loss: 0.0001397133\n",
      "Train epoch: 302, MSE_Loss: 0.0001393882\n",
      "Train epoch: 303, MSE_Loss: 0.0001394731\n",
      "Train epoch: 304, MSE_Loss: 0.0001388715\n",
      "Train epoch: 305, MSE_Loss: 0.0001400737\n",
      "Train epoch: 306, MSE_Loss: 0.0001395308\n",
      "Train epoch: 307, MSE_Loss: 0.0001389090\n",
      "Train epoch: 308, MSE_Loss: 0.0001391774\n",
      "Train epoch: 309, MSE_Loss: 0.0001381931\n",
      "Train epoch: 310, MSE_Loss: 0.0001398989\n",
      "Train epoch: 311, MSE_Loss: 0.0001394289\n",
      "Train epoch: 312, MSE_Loss: 0.0001380371\n",
      "Train epoch: 313, MSE_Loss: 0.0001392994\n",
      "Train epoch: 314, MSE_Loss: 0.0001402335\n",
      "Train epoch: 315, MSE_Loss: 0.0001394626\n",
      "Train epoch: 316, MSE_Loss: 0.0001390907\n",
      "Train epoch: 317, MSE_Loss: 0.0001395503\n",
      "Train epoch: 318, MSE_Loss: 0.0001390378\n",
      "Train epoch: 319, MSE_Loss: 0.0001390699\n",
      "Train epoch: 320, MSE_Loss: 0.0001380879\n",
      "Train epoch: 321, MSE_Loss: 0.0001388406\n",
      "Train epoch: 322, MSE_Loss: 0.0001377554\n",
      "Train epoch: 323, MSE_Loss: 0.0001388029\n",
      "Train epoch: 324, MSE_Loss: 0.0001403821\n",
      "Train epoch: 325, MSE_Loss: 0.0001391775\n",
      "Train epoch: 326, MSE_Loss: 0.0001390815\n",
      "Train epoch: 327, MSE_Loss: 0.0001372876\n",
      "Train epoch: 328, MSE_Loss: 0.0001381971\n",
      "Train epoch: 329, MSE_Loss: 0.0001370225\n",
      "Train epoch: 330, MSE_Loss: 0.0001388763\n",
      "Train epoch: 331, MSE_Loss: 0.0001395268\n",
      "Train epoch: 332, MSE_Loss: 0.0001377649\n",
      "Train epoch: 333, MSE_Loss: 0.0001386492\n",
      "Train epoch: 334, MSE_Loss: 0.0001369865\n",
      "Train epoch: 335, MSE_Loss: 0.0001368706\n",
      "Train epoch: 336, MSE_Loss: 0.0001373164\n",
      "Train epoch: 337, MSE_Loss: 0.0001371155\n",
      "Train epoch: 338, MSE_Loss: 0.0001386341\n",
      "Train epoch: 339, MSE_Loss: 0.0001372435\n",
      "Train epoch: 340, MSE_Loss: 0.0001382254\n",
      "Train epoch: 341, MSE_Loss: 0.0001376716\n",
      "Train epoch: 342, MSE_Loss: 0.0001389250\n",
      "Train epoch: 343, MSE_Loss: 0.0001377221\n",
      "Train epoch: 344, MSE_Loss: 0.0001383566\n",
      "Train epoch: 345, MSE_Loss: 0.0001374501\n",
      "Train epoch: 346, MSE_Loss: 0.0001375398\n",
      "Train epoch: 347, MSE_Loss: 0.0001366148\n",
      "Train epoch: 348, MSE_Loss: 0.0001388055\n",
      "Train epoch: 349, MSE_Loss: 0.0001378114\n",
      "Train epoch: 350, MSE_Loss: 0.0001366049\n",
      "Train epoch: 351, MSE_Loss: 0.0001383513\n",
      "Train epoch: 352, MSE_Loss: 0.0001365440\n",
      "Train epoch: 353, MSE_Loss: 0.0001378343\n",
      "Train epoch: 354, MSE_Loss: 0.0001373153\n",
      "Train epoch: 355, MSE_Loss: 0.0001378328\n",
      "Train epoch: 356, MSE_Loss: 0.0001379155\n",
      "Train epoch: 357, MSE_Loss: 0.0001360937\n",
      "Train epoch: 358, MSE_Loss: 0.0001371931\n",
      "Train epoch: 359, MSE_Loss: 0.0001372027\n",
      "Train epoch: 360, MSE_Loss: 0.0001383731\n",
      "Train epoch: 361, MSE_Loss: 0.0001400490\n",
      "Train epoch: 362, MSE_Loss: 0.0001380295\n",
      "Train epoch: 363, MSE_Loss: 0.0001362514\n",
      "Train epoch: 364, MSE_Loss: 0.0001391280\n",
      "Train epoch: 365, MSE_Loss: 0.0001363774\n",
      "Train epoch: 366, MSE_Loss: 0.0001373293\n",
      "Train epoch: 367, MSE_Loss: 0.0001363082\n",
      "Train epoch: 368, MSE_Loss: 0.0001369970\n",
      "Train epoch: 369, MSE_Loss: 0.0001372791\n",
      "Train epoch: 370, MSE_Loss: 0.0001375201\n",
      "Train epoch: 371, MSE_Loss: 0.0001368579\n",
      "Train epoch: 372, MSE_Loss: 0.0001359131\n",
      "Train epoch: 373, MSE_Loss: 0.0001374055\n",
      "Train epoch: 374, MSE_Loss: 0.0001370219\n",
      "Train epoch: 375, MSE_Loss: 0.0001358319\n",
      "Train epoch: 376, MSE_Loss: 0.0001371811\n",
      "Train epoch: 377, MSE_Loss: 0.0001353404\n",
      "Train epoch: 378, MSE_Loss: 0.0001364937\n",
      "Train epoch: 379, MSE_Loss: 0.0001351730\n",
      "Train epoch: 380, MSE_Loss: 0.0001383464\n",
      "Train epoch: 381, MSE_Loss: 0.0001365029\n",
      "Train epoch: 382, MSE_Loss: 0.0001362195\n",
      "Train epoch: 383, MSE_Loss: 0.0001378454\n",
      "Train epoch: 384, MSE_Loss: 0.0001351000\n",
      "Train epoch: 385, MSE_Loss: 0.0001378466\n",
      "Train epoch: 386, MSE_Loss: 0.0001391188\n",
      "Train epoch: 387, MSE_Loss: 0.0001354287\n",
      "Train epoch: 388, MSE_Loss: 0.0001358902\n",
      "Train epoch: 389, MSE_Loss: 0.0001360574\n",
      "Train epoch: 390, MSE_Loss: 0.0001349598\n",
      "Train epoch: 391, MSE_Loss: 0.0001350681\n",
      "Train epoch: 392, MSE_Loss: 0.0001366017\n",
      "Train epoch: 393, MSE_Loss: 0.0001348714\n",
      "Train epoch: 394, MSE_Loss: 0.0001343096\n",
      "Train epoch: 395, MSE_Loss: 0.0001350711\n",
      "Train epoch: 396, MSE_Loss: 0.0001392309\n",
      "Train epoch: 397, MSE_Loss: 0.0001345035\n",
      "Train epoch: 398, MSE_Loss: 0.0001344773\n",
      "Train epoch: 399, MSE_Loss: 0.0001361905\n",
      "Train epoch: 400, MSE_Loss: 0.0001346089\n",
      "Train epoch: 401, MSE_Loss: 0.0001368611\n",
      "Train epoch: 402, MSE_Loss: 0.0001351087\n",
      "Train epoch: 403, MSE_Loss: 0.0001355930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 404, MSE_Loss: 0.0001349531\n",
      "Train epoch: 405, MSE_Loss: 0.0001349722\n",
      "Train epoch: 406, MSE_Loss: 0.0001355453\n",
      "Train epoch: 407, MSE_Loss: 0.0001346323\n",
      "Train epoch: 408, MSE_Loss: 0.0001346487\n",
      "Train epoch: 409, MSE_Loss: 0.0001338638\n",
      "Train epoch: 410, MSE_Loss: 0.0001350068\n",
      "Train epoch: 411, MSE_Loss: 0.0001346319\n",
      "Train epoch: 412, MSE_Loss: 0.0001344511\n",
      "Train epoch: 413, MSE_Loss: 0.0001365894\n",
      "Train epoch: 414, MSE_Loss: 0.0001356254\n",
      "Train epoch: 415, MSE_Loss: 0.0001345432\n",
      "Train epoch: 416, MSE_Loss: 0.0001346580\n",
      "Train epoch: 417, MSE_Loss: 0.0001345201\n",
      "Train epoch: 418, MSE_Loss: 0.0001380192\n",
      "Train epoch: 419, MSE_Loss: 0.0001355555\n",
      "Train epoch: 420, MSE_Loss: 0.0001349087\n",
      "Train epoch: 421, MSE_Loss: 0.0001353484\n",
      "Train epoch: 422, MSE_Loss: 0.0001360539\n",
      "Train epoch: 423, MSE_Loss: 0.0001347968\n",
      "Train epoch: 424, MSE_Loss: 0.0001340656\n",
      "Train epoch: 425, MSE_Loss: 0.0001324095\n",
      "Train epoch: 426, MSE_Loss: 0.0001341019\n",
      "Train epoch: 427, MSE_Loss: 0.0001331132\n",
      "Train epoch: 428, MSE_Loss: 0.0001336316\n",
      "Train epoch: 429, MSE_Loss: 0.0001341081\n",
      "Train epoch: 430, MSE_Loss: 0.0001334464\n",
      "Train epoch: 431, MSE_Loss: 0.0001342390\n",
      "Train epoch: 432, MSE_Loss: 0.0001335803\n",
      "Train epoch: 433, MSE_Loss: 0.0001350203\n",
      "Train epoch: 434, MSE_Loss: 0.0001334558\n",
      "Train epoch: 435, MSE_Loss: 0.0001339225\n",
      "Train epoch: 436, MSE_Loss: 0.0001329348\n",
      "Train epoch: 437, MSE_Loss: 0.0001350523\n",
      "Train epoch: 438, MSE_Loss: 0.0001341984\n",
      "Train epoch: 439, MSE_Loss: 0.0001334079\n",
      "Train epoch: 440, MSE_Loss: 0.0001333122\n",
      "Train epoch: 441, MSE_Loss: 0.0001344453\n",
      "Train epoch: 442, MSE_Loss: 0.0001336674\n",
      "Train epoch: 443, MSE_Loss: 0.0001332439\n",
      "Train epoch: 444, MSE_Loss: 0.0001322781\n",
      "Train epoch: 445, MSE_Loss: 0.0001364079\n",
      "Train epoch: 446, MSE_Loss: 0.0001339102\n",
      "Train epoch: 447, MSE_Loss: 0.0001350165\n",
      "Train epoch: 448, MSE_Loss: 0.0001341588\n",
      "Train epoch: 449, MSE_Loss: 0.0001339737\n",
      "Train epoch: 450, MSE_Loss: 0.0001356259\n",
      "Train epoch: 451, MSE_Loss: 0.0001328942\n",
      "Train epoch: 452, MSE_Loss: 0.0001342875\n",
      "Train epoch: 453, MSE_Loss: 0.0001336335\n",
      "Train epoch: 454, MSE_Loss: 0.0001328339\n",
      "Train epoch: 455, MSE_Loss: 0.0001331751\n",
      "Train epoch: 456, MSE_Loss: 0.0001344183\n",
      "Train epoch: 457, MSE_Loss: 0.0001328552\n",
      "Train epoch: 458, MSE_Loss: 0.0001326765\n",
      "Train epoch: 459, MSE_Loss: 0.0001334434\n",
      "Train epoch: 460, MSE_Loss: 0.0001341328\n",
      "Train epoch: 461, MSE_Loss: 0.0001334587\n",
      "Train epoch: 462, MSE_Loss: 0.0001324704\n",
      "Train epoch: 463, MSE_Loss: 0.0001329585\n",
      "Train epoch: 464, MSE_Loss: 0.0001327294\n",
      "Train epoch: 465, MSE_Loss: 0.0001341464\n",
      "Train epoch: 466, MSE_Loss: 0.0001349012\n",
      "Train epoch: 467, MSE_Loss: 0.0001349190\n",
      "Train epoch: 468, MSE_Loss: 0.0001325367\n",
      "Train epoch: 469, MSE_Loss: 0.0001331582\n",
      "Train epoch: 470, MSE_Loss: 0.0001358427\n",
      "Train epoch: 471, MSE_Loss: 0.0001323311\n",
      "Train epoch: 472, MSE_Loss: 0.0001322517\n",
      "Train epoch: 473, MSE_Loss: 0.0001322242\n",
      "Train epoch: 474, MSE_Loss: 0.0001317472\n",
      "Train epoch: 475, MSE_Loss: 0.0001322599\n",
      "Train epoch: 476, MSE_Loss: 0.0001322801\n",
      "Train epoch: 477, MSE_Loss: 0.0001331507\n",
      "Train epoch: 478, MSE_Loss: 0.0001322900\n",
      "Train epoch: 479, MSE_Loss: 0.0001327181\n",
      "Train epoch: 480, MSE_Loss: 0.0001328205\n",
      "Train epoch: 481, MSE_Loss: 0.0001323531\n",
      "Train epoch: 482, MSE_Loss: 0.0001326889\n",
      "Train epoch: 483, MSE_Loss: 0.0001315159\n",
      "Train epoch: 484, MSE_Loss: 0.0001319145\n",
      "Train epoch: 485, MSE_Loss: 0.0001330333\n",
      "Train epoch: 486, MSE_Loss: 0.0001317517\n",
      "Train epoch: 487, MSE_Loss: 0.0001319186\n",
      "Train epoch: 488, MSE_Loss: 0.0001325114\n",
      "Train epoch: 489, MSE_Loss: 0.0001308838\n",
      "Train epoch: 490, MSE_Loss: 0.0001310476\n",
      "Train epoch: 491, MSE_Loss: 0.0001313744\n",
      "Train epoch: 492, MSE_Loss: 0.0001324302\n",
      "Train epoch: 493, MSE_Loss: 0.0001315824\n",
      "Train epoch: 494, MSE_Loss: 0.0001322816\n",
      "Train epoch: 495, MSE_Loss: 0.0001328011\n",
      "Train epoch: 496, MSE_Loss: 0.0001320036\n",
      "Train epoch: 497, MSE_Loss: 0.0001333644\n",
      "Train epoch: 498, MSE_Loss: 0.0001316196\n",
      "Train epoch: 499, MSE_Loss: 0.0001314476\n",
      "Train epoch: 500, MSE_Loss: 0.0001318978\n",
      "Train epoch: 501, MSE_Loss: 0.0001306249\n",
      "Train epoch: 502, MSE_Loss: 0.0001317121\n",
      "Train epoch: 503, MSE_Loss: 0.0001332499\n",
      "Train epoch: 504, MSE_Loss: 0.0001306168\n",
      "Train epoch: 505, MSE_Loss: 0.0001309636\n",
      "Train epoch: 506, MSE_Loss: 0.0001311222\n",
      "Train epoch: 507, MSE_Loss: 0.0001324336\n",
      "Train epoch: 508, MSE_Loss: 0.0001329030\n",
      "Train epoch: 509, MSE_Loss: 0.0001323156\n",
      "Train epoch: 510, MSE_Loss: 0.0001310673\n",
      "Train epoch: 511, MSE_Loss: 0.0001311460\n",
      "Train epoch: 512, MSE_Loss: 0.0001322178\n",
      "Train epoch: 513, MSE_Loss: 0.0001316379\n",
      "Train epoch: 514, MSE_Loss: 0.0001300261\n",
      "Train epoch: 515, MSE_Loss: 0.0001331504\n",
      "Train epoch: 516, MSE_Loss: 0.0001303308\n",
      "Train epoch: 517, MSE_Loss: 0.0001306387\n",
      "Train epoch: 518, MSE_Loss: 0.0001310782\n",
      "Train epoch: 519, MSE_Loss: 0.0001313067\n",
      "Train epoch: 520, MSE_Loss: 0.0001315682\n",
      "Train epoch: 521, MSE_Loss: 0.0001310903\n",
      "Train epoch: 522, MSE_Loss: 0.0001316241\n",
      "Train epoch: 523, MSE_Loss: 0.0001296757\n",
      "Train epoch: 524, MSE_Loss: 0.0001306895\n",
      "Train epoch: 525, MSE_Loss: 0.0001313283\n",
      "Train epoch: 526, MSE_Loss: 0.0001320581\n",
      "Train epoch: 527, MSE_Loss: 0.0001306565\n",
      "Train epoch: 528, MSE_Loss: 0.0001312503\n",
      "Train epoch: 529, MSE_Loss: 0.0001306140\n",
      "Train epoch: 530, MSE_Loss: 0.0001331251\n",
      "Train epoch: 531, MSE_Loss: 0.0001304567\n",
      "Train epoch: 532, MSE_Loss: 0.0001308304\n",
      "Train epoch: 533, MSE_Loss: 0.0001302568\n",
      "Train epoch: 534, MSE_Loss: 0.0001303105\n",
      "Train epoch: 535, MSE_Loss: 0.0001308921\n",
      "Train epoch: 536, MSE_Loss: 0.0001304906\n",
      "Train epoch: 537, MSE_Loss: 0.0001301221\n",
      "Train epoch: 538, MSE_Loss: 0.0001313768\n",
      "Train epoch: 539, MSE_Loss: 0.0001312749\n",
      "Train epoch: 540, MSE_Loss: 0.0001310123\n",
      "Train epoch: 541, MSE_Loss: 0.0001310826\n",
      "Train epoch: 542, MSE_Loss: 0.0001307543\n",
      "Train epoch: 543, MSE_Loss: 0.0001294578\n",
      "Train epoch: 544, MSE_Loss: 0.0001297441\n",
      "Train epoch: 545, MSE_Loss: 0.0001303861\n",
      "Train epoch: 546, MSE_Loss: 0.0001297306\n",
      "Train epoch: 547, MSE_Loss: 0.0001296689\n",
      "Train epoch: 548, MSE_Loss: 0.0001289357\n",
      "Train epoch: 549, MSE_Loss: 0.0001315855\n",
      "Train epoch: 550, MSE_Loss: 0.0001297400\n",
      "Train epoch: 551, MSE_Loss: 0.0001300227\n",
      "Train epoch: 552, MSE_Loss: 0.0001296761\n",
      "Train epoch: 553, MSE_Loss: 0.0001294347\n",
      "Train epoch: 554, MSE_Loss: 0.0001304653\n",
      "Train epoch: 555, MSE_Loss: 0.0001280195\n",
      "Train epoch: 556, MSE_Loss: 0.0001307667\n",
      "Train epoch: 557, MSE_Loss: 0.0001300333\n",
      "Train epoch: 558, MSE_Loss: 0.0001308978\n",
      "Train epoch: 559, MSE_Loss: 0.0001285273\n",
      "Train epoch: 560, MSE_Loss: 0.0001289754\n",
      "Train epoch: 561, MSE_Loss: 0.0001290663\n",
      "Train epoch: 562, MSE_Loss: 0.0001292660\n",
      "Train epoch: 563, MSE_Loss: 0.0001290884\n",
      "Train epoch: 564, MSE_Loss: 0.0001300548\n",
      "Train epoch: 565, MSE_Loss: 0.0001302176\n",
      "Train epoch: 566, MSE_Loss: 0.0001285496\n",
      "Train epoch: 567, MSE_Loss: 0.0001298742\n",
      "Train epoch: 568, MSE_Loss: 0.0001293530\n",
      "Train epoch: 569, MSE_Loss: 0.0001303446\n",
      "Train epoch: 570, MSE_Loss: 0.0001285430\n",
      "Train epoch: 571, MSE_Loss: 0.0001291467\n",
      "Train epoch: 572, MSE_Loss: 0.0001289143\n",
      "Train epoch: 573, MSE_Loss: 0.0001282196\n",
      "Train epoch: 574, MSE_Loss: 0.0001287492\n",
      "Train epoch: 575, MSE_Loss: 0.0001290780\n",
      "Train epoch: 576, MSE_Loss: 0.0001282793\n",
      "Train epoch: 577, MSE_Loss: 0.0001292139\n",
      "Train epoch: 578, MSE_Loss: 0.0001288583\n",
      "Train epoch: 579, MSE_Loss: 0.0001295885\n",
      "Train epoch: 580, MSE_Loss: 0.0001314198\n",
      "Train epoch: 581, MSE_Loss: 0.0001290911\n",
      "Train epoch: 582, MSE_Loss: 0.0001293108\n",
      "Train epoch: 583, MSE_Loss: 0.0001279273\n",
      "Train epoch: 584, MSE_Loss: 0.0001289370\n",
      "Train epoch: 585, MSE_Loss: 0.0001278260\n",
      "Train epoch: 586, MSE_Loss: 0.0001291854\n",
      "Train epoch: 587, MSE_Loss: 0.0001275866\n",
      "Train epoch: 588, MSE_Loss: 0.0001279539\n",
      "Train epoch: 589, MSE_Loss: 0.0001291049\n",
      "Train epoch: 590, MSE_Loss: 0.0001299534\n",
      "Train epoch: 591, MSE_Loss: 0.0001287365\n",
      "Train epoch: 592, MSE_Loss: 0.0001291281\n",
      "Train epoch: 593, MSE_Loss: 0.0001299306\n",
      "Train epoch: 594, MSE_Loss: 0.0001283855\n",
      "Train epoch: 595, MSE_Loss: 0.0001298621\n",
      "Train epoch: 596, MSE_Loss: 0.0001285811\n",
      "Train epoch: 597, MSE_Loss: 0.0001289799\n",
      "Train epoch: 598, MSE_Loss: 0.0001278283\n",
      "Train epoch: 599, MSE_Loss: 0.0001282458\n",
      "Train epoch: 600, MSE_Loss: 0.0001290018\n",
      "Train epoch: 601, MSE_Loss: 0.0001295203\n",
      "Train epoch: 602, MSE_Loss: 0.0001278948\n",
      "Train epoch: 603, MSE_Loss: 0.0001292703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 604, MSE_Loss: 0.0001279418\n",
      "Train epoch: 605, MSE_Loss: 0.0001289799\n",
      "Train epoch: 606, MSE_Loss: 0.0001284220\n",
      "Train epoch: 607, MSE_Loss: 0.0001296166\n",
      "Train epoch: 608, MSE_Loss: 0.0001277347\n",
      "Train epoch: 609, MSE_Loss: 0.0001279005\n",
      "Train epoch: 610, MSE_Loss: 0.0001271817\n",
      "Train epoch: 611, MSE_Loss: 0.0001300521\n",
      "Train epoch: 612, MSE_Loss: 0.0001292540\n",
      "Train epoch: 613, MSE_Loss: 0.0001282424\n",
      "Train epoch: 614, MSE_Loss: 0.0001268636\n",
      "Train epoch: 615, MSE_Loss: 0.0001278417\n",
      "Train epoch: 616, MSE_Loss: 0.0001269359\n",
      "Train epoch: 617, MSE_Loss: 0.0001275518\n",
      "Train epoch: 618, MSE_Loss: 0.0001292024\n",
      "Train epoch: 619, MSE_Loss: 0.0001278075\n",
      "Train epoch: 620, MSE_Loss: 0.0001291034\n",
      "Train epoch: 621, MSE_Loss: 0.0001282717\n",
      "Train epoch: 622, MSE_Loss: 0.0001280418\n",
      "Train epoch: 623, MSE_Loss: 0.0001266113\n",
      "Train epoch: 624, MSE_Loss: 0.0001288459\n",
      "Train epoch: 625, MSE_Loss: 0.0001264935\n",
      "Train epoch: 626, MSE_Loss: 0.0001279018\n",
      "Train epoch: 627, MSE_Loss: 0.0001273521\n",
      "Train epoch: 628, MSE_Loss: 0.0001265023\n",
      "Train epoch: 629, MSE_Loss: 0.0001301437\n",
      "Train epoch: 630, MSE_Loss: 0.0001274836\n",
      "Train epoch: 631, MSE_Loss: 0.0001276821\n",
      "Train epoch: 632, MSE_Loss: 0.0001261618\n",
      "Train epoch: 633, MSE_Loss: 0.0001272166\n",
      "Train epoch: 634, MSE_Loss: 0.0001266221\n",
      "Train epoch: 635, MSE_Loss: 0.0001270248\n",
      "Train epoch: 636, MSE_Loss: 0.0001263039\n",
      "Train epoch: 637, MSE_Loss: 0.0001267683\n",
      "Train epoch: 638, MSE_Loss: 0.0001281259\n",
      "Train epoch: 639, MSE_Loss: 0.0001277613\n",
      "Train epoch: 640, MSE_Loss: 0.0001271889\n",
      "Train epoch: 641, MSE_Loss: 0.0001270591\n",
      "Train epoch: 642, MSE_Loss: 0.0001279825\n",
      "Train epoch: 643, MSE_Loss: 0.0001271398\n",
      "Train epoch: 644, MSE_Loss: 0.0001265431\n",
      "Train epoch: 645, MSE_Loss: 0.0001275553\n",
      "Train epoch: 646, MSE_Loss: 0.0001276220\n",
      "Train epoch: 647, MSE_Loss: 0.0001264593\n",
      "Train epoch: 648, MSE_Loss: 0.0001261707\n",
      "Train epoch: 649, MSE_Loss: 0.0001255106\n",
      "Train epoch: 650, MSE_Loss: 0.0001259432\n",
      "Train epoch: 651, MSE_Loss: 0.0001276662\n",
      "Train epoch: 652, MSE_Loss: 0.0001284438\n",
      "Train epoch: 653, MSE_Loss: 0.0001272073\n",
      "Train epoch: 654, MSE_Loss: 0.0001262448\n",
      "Train epoch: 655, MSE_Loss: 0.0001261811\n",
      "Train epoch: 656, MSE_Loss: 0.0001252132\n",
      "Train epoch: 657, MSE_Loss: 0.0001258740\n",
      "Train epoch: 658, MSE_Loss: 0.0001281875\n",
      "Train epoch: 659, MSE_Loss: 0.0001278840\n",
      "Train epoch: 660, MSE_Loss: 0.0001280348\n",
      "Train epoch: 661, MSE_Loss: 0.0001261766\n",
      "Train epoch: 662, MSE_Loss: 0.0001259827\n",
      "Train epoch: 663, MSE_Loss: 0.0001285244\n",
      "Train epoch: 664, MSE_Loss: 0.0001267468\n",
      "Train epoch: 665, MSE_Loss: 0.0001264447\n",
      "Train epoch: 666, MSE_Loss: 0.0001259792\n",
      "Train epoch: 667, MSE_Loss: 0.0001265948\n",
      "Train epoch: 668, MSE_Loss: 0.0001266870\n",
      "Train epoch: 669, MSE_Loss: 0.0001260124\n",
      "Train epoch: 670, MSE_Loss: 0.0001259221\n",
      "Train epoch: 671, MSE_Loss: 0.0001268687\n",
      "Train epoch: 672, MSE_Loss: 0.0001256359\n",
      "Train epoch: 673, MSE_Loss: 0.0001253434\n",
      "Train epoch: 674, MSE_Loss: 0.0001258922\n",
      "Train epoch: 675, MSE_Loss: 0.0001265670\n",
      "Train epoch: 676, MSE_Loss: 0.0001261887\n",
      "Train epoch: 677, MSE_Loss: 0.0001259933\n",
      "Train epoch: 678, MSE_Loss: 0.0001255761\n",
      "Train epoch: 679, MSE_Loss: 0.0001267108\n",
      "Train epoch: 680, MSE_Loss: 0.0001277212\n",
      "Train epoch: 681, MSE_Loss: 0.0001265486\n",
      "Train epoch: 682, MSE_Loss: 0.0001253612\n",
      "Train epoch: 683, MSE_Loss: 0.0001268141\n",
      "Train epoch: 684, MSE_Loss: 0.0001261907\n",
      "Train epoch: 685, MSE_Loss: 0.0001279770\n",
      "Train epoch: 686, MSE_Loss: 0.0001254316\n",
      "Train epoch: 687, MSE_Loss: 0.0001253060\n",
      "Train epoch: 688, MSE_Loss: 0.0001244606\n",
      "Train epoch: 689, MSE_Loss: 0.0001255236\n",
      "Train epoch: 690, MSE_Loss: 0.0001276842\n",
      "Train epoch: 691, MSE_Loss: 0.0001264142\n",
      "Train epoch: 692, MSE_Loss: 0.0001281027\n",
      "Train epoch: 693, MSE_Loss: 0.0001275140\n",
      "Train epoch: 694, MSE_Loss: 0.0001248195\n",
      "Train epoch: 695, MSE_Loss: 0.0001252318\n",
      "Train epoch: 696, MSE_Loss: 0.0001249922\n",
      "Train epoch: 697, MSE_Loss: 0.0001255174\n",
      "Train epoch: 698, MSE_Loss: 0.0001259563\n",
      "Train epoch: 699, MSE_Loss: 0.0001258792\n",
      "Train epoch: 700, MSE_Loss: 0.0001258399\n",
      "Train epoch: 701, MSE_Loss: 0.0001241726\n",
      "Train epoch: 702, MSE_Loss: 0.0001245452\n",
      "Train epoch: 703, MSE_Loss: 0.0001250462\n",
      "Train epoch: 704, MSE_Loss: 0.0001238864\n",
      "Train epoch: 705, MSE_Loss: 0.0001244327\n",
      "Train epoch: 706, MSE_Loss: 0.0001257983\n",
      "Train epoch: 707, MSE_Loss: 0.0001253806\n",
      "Train epoch: 708, MSE_Loss: 0.0001247252\n",
      "Train epoch: 709, MSE_Loss: 0.0001243160\n",
      "Train epoch: 710, MSE_Loss: 0.0001257055\n",
      "Train epoch: 711, MSE_Loss: 0.0001252448\n",
      "Train epoch: 712, MSE_Loss: 0.0001273531\n",
      "Train epoch: 713, MSE_Loss: 0.0001248874\n",
      "Train epoch: 714, MSE_Loss: 0.0001237296\n",
      "Train epoch: 715, MSE_Loss: 0.0001236474\n",
      "Train epoch: 716, MSE_Loss: 0.0001253294\n",
      "Train epoch: 717, MSE_Loss: 0.0001253406\n",
      "Train epoch: 718, MSE_Loss: 0.0001249042\n",
      "Train epoch: 719, MSE_Loss: 0.0001245654\n",
      "Train epoch: 720, MSE_Loss: 0.0001244698\n",
      "Train epoch: 721, MSE_Loss: 0.0001244182\n",
      "Train epoch: 722, MSE_Loss: 0.0001242299\n",
      "Train epoch: 723, MSE_Loss: 0.0001244099\n",
      "Train epoch: 724, MSE_Loss: 0.0001238096\n",
      "Train epoch: 725, MSE_Loss: 0.0001259387\n",
      "Train epoch: 726, MSE_Loss: 0.0001230982\n",
      "Train epoch: 727, MSE_Loss: 0.0001242294\n",
      "Train epoch: 728, MSE_Loss: 0.0001231710\n",
      "Train epoch: 729, MSE_Loss: 0.0001238466\n",
      "Train epoch: 730, MSE_Loss: 0.0001249459\n",
      "Train epoch: 731, MSE_Loss: 0.0001249272\n",
      "Train epoch: 732, MSE_Loss: 0.0001245736\n",
      "Train epoch: 733, MSE_Loss: 0.0001242869\n",
      "Train epoch: 734, MSE_Loss: 0.0001237359\n",
      "Train epoch: 735, MSE_Loss: 0.0001230216\n",
      "Train epoch: 736, MSE_Loss: 0.0001266778\n",
      "Train epoch: 737, MSE_Loss: 0.0001247724\n",
      "Train epoch: 738, MSE_Loss: 0.0001225490\n",
      "Train epoch: 739, MSE_Loss: 0.0001239039\n",
      "Train epoch: 740, MSE_Loss: 0.0001244178\n",
      "Train epoch: 741, MSE_Loss: 0.0001223268\n",
      "Train epoch: 742, MSE_Loss: 0.0001234540\n",
      "Train epoch: 743, MSE_Loss: 0.0001243117\n",
      "Train epoch: 744, MSE_Loss: 0.0001240128\n",
      "Train epoch: 745, MSE_Loss: 0.0001247532\n",
      "Train epoch: 746, MSE_Loss: 0.0001227886\n",
      "Train epoch: 747, MSE_Loss: 0.0001237328\n",
      "Train epoch: 748, MSE_Loss: 0.0001245434\n",
      "Train epoch: 749, MSE_Loss: 0.0001229276\n",
      "Train epoch: 750, MSE_Loss: 0.0001245670\n",
      "Train epoch: 751, MSE_Loss: 0.0001231396\n",
      "Train epoch: 752, MSE_Loss: 0.0001233328\n",
      "Train epoch: 753, MSE_Loss: 0.0001251281\n",
      "Train epoch: 754, MSE_Loss: 0.0001231953\n",
      "Train epoch: 755, MSE_Loss: 0.0001238891\n",
      "Train epoch: 756, MSE_Loss: 0.0001237182\n",
      "Train epoch: 757, MSE_Loss: 0.0001229347\n",
      "Train epoch: 758, MSE_Loss: 0.0001230576\n",
      "Train epoch: 759, MSE_Loss: 0.0001224240\n",
      "Train epoch: 760, MSE_Loss: 0.0001239387\n",
      "Train epoch: 761, MSE_Loss: 0.0001232273\n",
      "Train epoch: 762, MSE_Loss: 0.0001239848\n",
      "Train epoch: 763, MSE_Loss: 0.0001242098\n",
      "Train epoch: 764, MSE_Loss: 0.0001234133\n",
      "Train epoch: 765, MSE_Loss: 0.0001240725\n",
      "Train epoch: 766, MSE_Loss: 0.0001224657\n",
      "Train epoch: 767, MSE_Loss: 0.0001232857\n",
      "Train epoch: 768, MSE_Loss: 0.0001232263\n",
      "Train epoch: 769, MSE_Loss: 0.0001223975\n",
      "Train epoch: 770, MSE_Loss: 0.0001236025\n",
      "Train epoch: 771, MSE_Loss: 0.0001223055\n",
      "Train epoch: 772, MSE_Loss: 0.0001218519\n",
      "Train epoch: 773, MSE_Loss: 0.0001218805\n",
      "Train epoch: 774, MSE_Loss: 0.0001240572\n",
      "Train epoch: 775, MSE_Loss: 0.0001222715\n",
      "Train epoch: 776, MSE_Loss: 0.0001219908\n",
      "Train epoch: 777, MSE_Loss: 0.0001240774\n",
      "Train epoch: 778, MSE_Loss: 0.0001216859\n",
      "Train epoch: 779, MSE_Loss: 0.0001216915\n",
      "Train epoch: 780, MSE_Loss: 0.0001221244\n",
      "Train epoch: 781, MSE_Loss: 0.0001220471\n",
      "Train epoch: 782, MSE_Loss: 0.0001218126\n",
      "Train epoch: 783, MSE_Loss: 0.0001220764\n",
      "Train epoch: 784, MSE_Loss: 0.0001222500\n",
      "Train epoch: 785, MSE_Loss: 0.0001222920\n",
      "Train epoch: 786, MSE_Loss: 0.0001230004\n",
      "Train epoch: 787, MSE_Loss: 0.0001231625\n",
      "Train epoch: 788, MSE_Loss: 0.0001219815\n",
      "Train epoch: 789, MSE_Loss: 0.0001238581\n",
      "Train epoch: 790, MSE_Loss: 0.0001213398\n",
      "Train epoch: 791, MSE_Loss: 0.0001216194\n",
      "Train epoch: 792, MSE_Loss: 0.0001218512\n",
      "Train epoch: 793, MSE_Loss: 0.0001227118\n",
      "Train epoch: 794, MSE_Loss: 0.0001225818\n",
      "Train epoch: 795, MSE_Loss: 0.0001215373\n",
      "Train epoch: 796, MSE_Loss: 0.0001217155\n",
      "Train epoch: 797, MSE_Loss: 0.0001214049\n",
      "Train epoch: 798, MSE_Loss: 0.0001226330\n",
      "Train epoch: 799, MSE_Loss: 0.0001229423\n",
      "Train epoch: 800, MSE_Loss: 0.0001211450\n",
      "Train epoch: 801, MSE_Loss: 0.0001213849\n",
      "Train epoch: 802, MSE_Loss: 0.0001210174\n",
      "Train epoch: 803, MSE_Loss: 0.0001232648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 804, MSE_Loss: 0.0001242554\n",
      "Train epoch: 805, MSE_Loss: 0.0001223979\n",
      "Train epoch: 806, MSE_Loss: 0.0001213457\n",
      "Train epoch: 807, MSE_Loss: 0.0001213532\n",
      "Train epoch: 808, MSE_Loss: 0.0001225344\n",
      "Train epoch: 809, MSE_Loss: 0.0001208133\n",
      "Train epoch: 810, MSE_Loss: 0.0001210360\n",
      "Train epoch: 811, MSE_Loss: 0.0001215122\n",
      "Train epoch: 812, MSE_Loss: 0.0001212055\n",
      "Train epoch: 813, MSE_Loss: 0.0001225083\n",
      "Train epoch: 814, MSE_Loss: 0.0001217597\n",
      "Train epoch: 815, MSE_Loss: 0.0001222320\n",
      "Train epoch: 816, MSE_Loss: 0.0001213628\n",
      "Train epoch: 817, MSE_Loss: 0.0001222625\n",
      "Train epoch: 818, MSE_Loss: 0.0001216425\n",
      "Train epoch: 819, MSE_Loss: 0.0001220874\n",
      "Train epoch: 820, MSE_Loss: 0.0001216565\n",
      "Train epoch: 821, MSE_Loss: 0.0001201772\n",
      "Train epoch: 822, MSE_Loss: 0.0001213894\n",
      "Train epoch: 823, MSE_Loss: 0.0001216005\n",
      "Train epoch: 824, MSE_Loss: 0.0001214611\n",
      "Train epoch: 825, MSE_Loss: 0.0001244738\n",
      "Train epoch: 826, MSE_Loss: 0.0001215219\n",
      "Train epoch: 827, MSE_Loss: 0.0001209948\n",
      "Train epoch: 828, MSE_Loss: 0.0001221325\n",
      "Train epoch: 829, MSE_Loss: 0.0001212194\n",
      "Train epoch: 830, MSE_Loss: 0.0001215412\n",
      "Train epoch: 831, MSE_Loss: 0.0001227990\n",
      "Train epoch: 832, MSE_Loss: 0.0001208968\n",
      "Train epoch: 833, MSE_Loss: 0.0001222860\n",
      "Train epoch: 834, MSE_Loss: 0.0001205004\n",
      "Train epoch: 835, MSE_Loss: 0.0001213639\n",
      "Train epoch: 836, MSE_Loss: 0.0001202699\n",
      "Train epoch: 837, MSE_Loss: 0.0001214790\n",
      "Train epoch: 838, MSE_Loss: 0.0001216542\n",
      "Train epoch: 839, MSE_Loss: 0.0001197795\n",
      "Train epoch: 840, MSE_Loss: 0.0001212455\n",
      "Train epoch: 841, MSE_Loss: 0.0001205725\n",
      "Train epoch: 842, MSE_Loss: 0.0001214753\n",
      "Train epoch: 843, MSE_Loss: 0.0001202090\n",
      "Train epoch: 844, MSE_Loss: 0.0001199276\n",
      "Train epoch: 845, MSE_Loss: 0.0001209965\n",
      "Train epoch: 846, MSE_Loss: 0.0001198423\n",
      "Train epoch: 847, MSE_Loss: 0.0001200371\n",
      "Train epoch: 848, MSE_Loss: 0.0001214414\n",
      "Train epoch: 849, MSE_Loss: 0.0001202977\n",
      "Train epoch: 850, MSE_Loss: 0.0001188618\n",
      "Train epoch: 851, MSE_Loss: 0.0001214052\n",
      "Train epoch: 852, MSE_Loss: 0.0001208681\n",
      "Train epoch: 853, MSE_Loss: 0.0001192131\n",
      "Train epoch: 854, MSE_Loss: 0.0001194692\n",
      "Train epoch: 855, MSE_Loss: 0.0001198148\n",
      "Train epoch: 856, MSE_Loss: 0.0001211169\n",
      "Train epoch: 857, MSE_Loss: 0.0001184030\n",
      "Train epoch: 858, MSE_Loss: 0.0001193924\n",
      "Train epoch: 859, MSE_Loss: 0.0001189782\n",
      "Train epoch: 860, MSE_Loss: 0.0001201495\n",
      "Train epoch: 861, MSE_Loss: 0.0001207275\n",
      "Train epoch: 862, MSE_Loss: 0.0001199658\n",
      "Train epoch: 863, MSE_Loss: 0.0001193924\n",
      "Train epoch: 864, MSE_Loss: 0.0001207018\n",
      "Train epoch: 865, MSE_Loss: 0.0001200757\n",
      "Train epoch: 866, MSE_Loss: 0.0001192616\n",
      "Train epoch: 867, MSE_Loss: 0.0001201801\n",
      "Train epoch: 868, MSE_Loss: 0.0001191559\n",
      "Train epoch: 869, MSE_Loss: 0.0001185741\n",
      "Train epoch: 870, MSE_Loss: 0.0001201886\n",
      "Train epoch: 871, MSE_Loss: 0.0001187910\n",
      "Train epoch: 872, MSE_Loss: 0.0001212056\n",
      "Train epoch: 873, MSE_Loss: 0.0001204890\n",
      "Train epoch: 874, MSE_Loss: 0.0001198480\n",
      "Train epoch: 875, MSE_Loss: 0.0001187346\n",
      "Train epoch: 876, MSE_Loss: 0.0001194618\n",
      "Train epoch: 877, MSE_Loss: 0.0001201147\n",
      "Train epoch: 878, MSE_Loss: 0.0001183875\n",
      "Train epoch: 879, MSE_Loss: 0.0001197242\n",
      "Train epoch: 880, MSE_Loss: 0.0001185230\n",
      "Train epoch: 881, MSE_Loss: 0.0001185759\n",
      "Train epoch: 882, MSE_Loss: 0.0001183483\n",
      "Train epoch: 883, MSE_Loss: 0.0001196307\n",
      "Train epoch: 884, MSE_Loss: 0.0001198649\n",
      "Train epoch: 885, MSE_Loss: 0.0001193962\n",
      "Train epoch: 886, MSE_Loss: 0.0001188265\n",
      "Train epoch: 887, MSE_Loss: 0.0001205337\n",
      "Train epoch: 888, MSE_Loss: 0.0001216103\n",
      "Train epoch: 889, MSE_Loss: 0.0001192935\n",
      "Train epoch: 890, MSE_Loss: 0.0001189554\n",
      "Train epoch: 891, MSE_Loss: 0.0001179424\n",
      "Train epoch: 892, MSE_Loss: 0.0001188967\n",
      "Train epoch: 893, MSE_Loss: 0.0001189126\n",
      "Train epoch: 894, MSE_Loss: 0.0001183377\n",
      "Train epoch: 895, MSE_Loss: 0.0001202031\n",
      "Train epoch: 896, MSE_Loss: 0.0001184169\n",
      "Train epoch: 897, MSE_Loss: 0.0001186676\n",
      "Train epoch: 898, MSE_Loss: 0.0001178131\n",
      "Train epoch: 899, MSE_Loss: 0.0001168188\n",
      "Train epoch: 900, MSE_Loss: 0.0001179853\n",
      "Train epoch: 901, MSE_Loss: 0.0001187535\n",
      "Train epoch: 902, MSE_Loss: 0.0001216964\n",
      "Train epoch: 903, MSE_Loss: 0.0001198225\n",
      "Train epoch: 904, MSE_Loss: 0.0001182340\n",
      "Train epoch: 905, MSE_Loss: 0.0001199165\n",
      "Train epoch: 906, MSE_Loss: 0.0001213293\n",
      "Train epoch: 907, MSE_Loss: 0.0001177073\n",
      "Train epoch: 908, MSE_Loss: 0.0001179184\n",
      "Train epoch: 909, MSE_Loss: 0.0001171678\n",
      "Train epoch: 910, MSE_Loss: 0.0001184743\n",
      "Train epoch: 911, MSE_Loss: 0.0001191823\n",
      "Train epoch: 912, MSE_Loss: 0.0001178063\n",
      "Train epoch: 913, MSE_Loss: 0.0001167810\n",
      "Train epoch: 914, MSE_Loss: 0.0001187820\n",
      "Train epoch: 915, MSE_Loss: 0.0001170956\n",
      "Train epoch: 916, MSE_Loss: 0.0001201499\n",
      "Train epoch: 917, MSE_Loss: 0.0001189813\n",
      "Train epoch: 918, MSE_Loss: 0.0001166685\n",
      "Train epoch: 919, MSE_Loss: 0.0001174485\n",
      "Train epoch: 920, MSE_Loss: 0.0001178033\n",
      "Train epoch: 921, MSE_Loss: 0.0001176274\n",
      "Train epoch: 922, MSE_Loss: 0.0001167267\n",
      "Train epoch: 923, MSE_Loss: 0.0001182895\n",
      "Train epoch: 924, MSE_Loss: 0.0001182532\n",
      "Train epoch: 925, MSE_Loss: 0.0001166882\n",
      "Train epoch: 926, MSE_Loss: 0.0001186656\n",
      "Train epoch: 927, MSE_Loss: 0.0001190412\n",
      "Train epoch: 928, MSE_Loss: 0.0001168413\n",
      "Train epoch: 929, MSE_Loss: 0.0001184154\n",
      "Train epoch: 930, MSE_Loss: 0.0001174417\n",
      "Train epoch: 931, MSE_Loss: 0.0001190203\n",
      "Train epoch: 932, MSE_Loss: 0.0001175519\n",
      "Train epoch: 933, MSE_Loss: 0.0001180072\n",
      "Train epoch: 934, MSE_Loss: 0.0001182756\n",
      "Train epoch: 935, MSE_Loss: 0.0001192331\n",
      "Train epoch: 936, MSE_Loss: 0.0001172992\n",
      "Train epoch: 937, MSE_Loss: 0.0001179330\n",
      "Train epoch: 938, MSE_Loss: 0.0001161558\n",
      "Train epoch: 939, MSE_Loss: 0.0001179041\n",
      "Train epoch: 940, MSE_Loss: 0.0001172554\n",
      "Train epoch: 941, MSE_Loss: 0.0001166835\n",
      "Train epoch: 942, MSE_Loss: 0.0001181217\n",
      "Train epoch: 943, MSE_Loss: 0.0001162236\n",
      "Train epoch: 944, MSE_Loss: 0.0001170313\n",
      "Train epoch: 945, MSE_Loss: 0.0001171078\n",
      "Train epoch: 946, MSE_Loss: 0.0001183644\n",
      "Train epoch: 947, MSE_Loss: 0.0001182231\n",
      "Train epoch: 948, MSE_Loss: 0.0001196202\n",
      "Train epoch: 949, MSE_Loss: 0.0001170187\n",
      "Train epoch: 950, MSE_Loss: 0.0001176666\n",
      "Train epoch: 951, MSE_Loss: 0.0001165946\n",
      "Train epoch: 952, MSE_Loss: 0.0001169201\n",
      "Train epoch: 953, MSE_Loss: 0.0001166260\n",
      "Train epoch: 954, MSE_Loss: 0.0001180320\n",
      "Train epoch: 955, MSE_Loss: 0.0001172305\n",
      "Train epoch: 956, MSE_Loss: 0.0001174467\n",
      "Train epoch: 957, MSE_Loss: 0.0001185782\n",
      "Train epoch: 958, MSE_Loss: 0.0001174290\n",
      "Train epoch: 959, MSE_Loss: 0.0001160056\n",
      "Train epoch: 960, MSE_Loss: 0.0001176845\n",
      "Train epoch: 961, MSE_Loss: 0.0001178155\n",
      "Train epoch: 962, MSE_Loss: 0.0001182144\n",
      "Train epoch: 963, MSE_Loss: 0.0001168548\n",
      "Train epoch: 964, MSE_Loss: 0.0001175020\n",
      "Train epoch: 965, MSE_Loss: 0.0001163200\n",
      "Train epoch: 966, MSE_Loss: 0.0001186102\n",
      "Train epoch: 967, MSE_Loss: 0.0001171142\n",
      "Train epoch: 968, MSE_Loss: 0.0001169015\n",
      "Train epoch: 969, MSE_Loss: 0.0001173681\n",
      "Train epoch: 970, MSE_Loss: 0.0001183834\n",
      "Train epoch: 971, MSE_Loss: 0.0001152331\n",
      "Train epoch: 972, MSE_Loss: 0.0001181397\n",
      "Train epoch: 973, MSE_Loss: 0.0001169425\n",
      "Train epoch: 974, MSE_Loss: 0.0001166390\n",
      "Train epoch: 975, MSE_Loss: 0.0001160996\n",
      "Train epoch: 976, MSE_Loss: 0.0001169847\n",
      "Train epoch: 977, MSE_Loss: 0.0001154733\n",
      "Train epoch: 978, MSE_Loss: 0.0001163721\n",
      "Train epoch: 979, MSE_Loss: 0.0001159759\n",
      "Train epoch: 980, MSE_Loss: 0.0001166150\n",
      "Train epoch: 981, MSE_Loss: 0.0001168857\n",
      "Train epoch: 982, MSE_Loss: 0.0001152201\n",
      "Train epoch: 983, MSE_Loss: 0.0001190519\n",
      "Train epoch: 984, MSE_Loss: 0.0001160820\n",
      "Train epoch: 985, MSE_Loss: 0.0001157370\n",
      "Train epoch: 986, MSE_Loss: 0.0001153597\n",
      "Train epoch: 987, MSE_Loss: 0.0001159978\n",
      "Train epoch: 988, MSE_Loss: 0.0001157962\n",
      "Train epoch: 989, MSE_Loss: 0.0001157294\n",
      "Train epoch: 990, MSE_Loss: 0.0001169893\n",
      "Train epoch: 991, MSE_Loss: 0.0001162272\n",
      "Train epoch: 992, MSE_Loss: 0.0001151865\n",
      "Train epoch: 993, MSE_Loss: 0.0001154690\n",
      "Train epoch: 994, MSE_Loss: 0.0001155874\n",
      "Train epoch: 995, MSE_Loss: 0.0001171379\n",
      "Train epoch: 996, MSE_Loss: 0.0001160624\n",
      "Train epoch: 997, MSE_Loss: 0.0001169166\n",
      "Train epoch: 998, MSE_Loss: 0.0001149375\n",
      "Train epoch: 999, MSE_Loss: 0.0001161876\n",
      "Train epoch: 1000, MSE_Loss: 0.0001169633\n",
      "Finished Training\n",
      "Wall time: 45min 48s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "trainTestLog = open(\"RegTrainTestLog1000.txt\",\"w+\") \n",
    "n_epoch = 1000\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "#epoch = 0\n",
    "#while True:\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda() #Reg\n",
    "        #inputs, labels = data[0].cuda(), data[2].cuda() #Color\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "    logStr = 'Train epoch: %d, MSE_Loss: %.10f' % (epoch + 1, train_loss/ (i+1))\n",
    "    trainTestLog.write(logStr+'\\n') #Save training records into log.\n",
    "    print(logStr)\n",
    "    epoch += 1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proprietary-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './RegWithLR0.1EP1000.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valued-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of the network on the 75 test samples: 0.0001194611\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testLoader, 0):\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda() #Reg\n",
    "        #inputs, labels = data[0].cuda(), data[2].cuda() #Color\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print statistics\n",
    "        test_loss += loss.item()       \n",
    "    n_samples = (i+1)*len(outputs)       \n",
    "\n",
    "logStr = 'Test MSE of the network on the %d test samples: %.10f' % (n_samples, test_loss/(i+1))\n",
    "trainTestLog.write(logStr+'\\n') #Save training records into log.\n",
    "print(logStr)\n",
    "trainTestLog.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-sunday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
